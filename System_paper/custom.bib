@article{Hendrycks2023,
    author = {Hendrycks, Dan and Mazeika, Mantas and Woodside, Thomas},
    copyright = {http://arxiv.org/licenses/nonexclusive-distrib/1.0},
    language = {eng},
    title = {An overview of catastrophic {AI} risks},
    year = {2023},
    }

@article{motoki2024,
  title={More human than human: measuring ChatGPT political bias},
  author={Motoki, Fabio and Pinho Neto, Valdemar and Rodrigues, Victor},
  journal={Public Choice},
  volume={198},
  number={1},
  pages={3--23},
  year={2024},
  publisher={Springer}
}

@article{wen2023,
  title={Unveiling the implicit toxicity in large language models},
  author={Wen, Jiaxin and Ke, Pei and Sun, Hao and Zhang, Zhexin and Li, Chengfei and Bai, Jinfeng and Huang, Minlie},
  journal={arXiv preprint arXiv:2311.17391},
  year={2023}
}

@article{zou2023,
  title={Universal and transferable adversarial attacks on aligned language models},
  author={Zou, Andy and Wang, Zifan and Carlini, Nicholas and Nasr, Milad and Kolter, J Zico and Fredrikson, Matt},
  journal={arXiv preprint arXiv:2307.15043},
  year={2023}
}

@article{brown2020,
  title={Language models are few-shot learners},
  author={Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and others},
  journal={Advances in neural information processing systems},
  volume={33},
  pages={1877--1901},
  year={2020}
}

@article{nguyen2022survey,
  title={A survey of machine unlearning},
  author={Nguyen, Thanh Tam and Huynh, Thanh Trung and Ren, Zhao and Nguyen, Phi Le and Liew, Alan Wee-Chung and Yin, Hongzhi and Nguyen, Quoc Viet Hung},
  journal={arXiv preprint arXiv:2209.02299},
  year={2022}
}

@article{yao2023,
  title={Large language model unlearning},
  author={Yao, Yuanshun and Xu, Xiaojun and Liu, Yang},
  journal={arXiv preprint arXiv:2310.10683},
  year={2023}
}

@article{bu2024,
  title={Unlearning as multi-task optimization: A normalized gradient difference approach with an adaptive learning rate},
  author={Bu, Zhiqi and Jin, Xiaomeng and Vinzamuri, Bhanukiran and Ramakrishna, Anil and Chang, Kai-Wei and Cevher, Volkan and Hong, Mingyi},
  journal={arXiv preprint arXiv:2410.22086},
  year={2024}
}

@article{liu2025rethinking,
  title={Rethinking machine unlearning for large language models},
  author={Liu, Sijia and Yao, Yuanshun and Jia, Jinghan and Casper, Stephen and Baracaldo, Nathalie and Hase, Peter and Yao, Yuguang and Liu, Chris Yuhao and Xu, Xiaojun and Li, Hang and others},
  journal={Nature Machine Intelligence},
  pages={1--14},
  year={2025},
  publisher={Nature Publishing Group UK London}
}

@article{jang2022knowledge,
  title={Knowledge unlearning for mitigating privacy risks in language models},
  author={Jang, Joel and Yoon, Dongkeun and Yang, Sohee and Cha, Sungmin and Lee, Moontae and Logeswaran, Lajanugen and Seo, Minjoon},
  journal={arXiv preprint arXiv:2210.01504},
  year={2022}
}

@inproceedings{marchant2022hard,
  title={Hard to forget: Poisoning attacks on certified machine unlearning},
  author={Marchant, Neil G and Rubinstein, Benjamin IP and Alfeld, Scott},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={36},
  number={7},
  pages={7691--7700},
  year={2022}
}

@article{singh2017data,
  title={Data leakage detection using cloud computing},
  author={Singh, Abhijeet and Anand, Abhineet},
  journal={International Journal Of Engineering And Computer Science},
  volume={6},
  number={4},
  year={2017}
}

@article{chen2023unlearn,
  title={Unlearn what you want to forget: Efficient unlearning for llms},
  author={Chen, Jiaao and Yang, Diyi},
  journal={arXiv preprint arXiv:2310.20150},
  year={2023}
}

@article{touvron2023open,
  title={Open and efficient foundation language models},
  author={Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timoth{\'e}e and Rozi{\`e}re, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and others},
  journal={Preprint at arXiv. https://doi. org/10.48550/arXiv},
  volume={2302},
  number={3},
  year={2023}
}

@inproceedings{neel2021descent,
  title={Descent-to-delete: Gradient-based methods for machine unlearning},
  author={Neel, Seth and Roth, Aaron and Sharifi-Malvajerdi, Saeed},
  booktitle={Algorithmic Learning Theory},
  pages={931--962},
  year={2021},
  organization={PMLR}
}

@article{trippa2024tau,
  title={âˆ‡ $\tau$: Gradient-based and Task-Agnostic machine Unlearning},
  author={Trippa, Daniel and Campagnano, Cesare and Bucarelli, Maria Sofia and Tolomei, Gabriele and Silvestri, Fabrizio},
  journal={CoRR},
  year={2024}
}

@article{Xu2023MachineUnlearning,
  author    = {Heng Xu and Tianqing Zhu and Lefeng Zhang and Wanlei Zhou and Philip S. Yu},
  title     = {Machine Unlearning: A Survey},
  journal   = {ACM Computing Surveys},
  volume    = {56},
  number    = {1},
  article   = {9},
  year      = {2023},
  month     = {August},
  pages     = {36},
  doi       = {10.1145/3603620}
}

@article{maini2024tofu,
  title={Tofu: A task of fictitious unlearning for llms},
  author={Maini, Pratyush and Feng, Zhili and Schwarzschild, Avi and Lipton, Zachary C and Kolter, J Zico},
  journal={arXiv preprint arXiv:2401.06121},
  year={2024}
}


@article{guo2019certified,
  title={Certified data removal from machine learning models},
  author={Guo, Chuan and Goldstein, Tom and Hannun, Awni and Van Der Maaten, Laurens},
  journal={arXiv preprint arXiv:1911.03030},
  year={2019}
}